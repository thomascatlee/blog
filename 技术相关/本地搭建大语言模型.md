---
title: 本地搭建大语言模型
date: 2024-03-12 14:54:24 +0800
author: wildcat
tags:
---
之前用[mlc-chat](https://github.com/mlc-ai/mlc-llm)搞过一个本地的大语言模型，只是其号称是支持

| AMD GPU      | NVIDIA GPU                    | Apple GPU      | Intel GPU            |                |
| ------------ | ----------------------------- | -------------- | -------------------- | -------------- |
| Linux / Win  | ✅ Vulkan, ROCm                | ✅ Vulkan, CUDA | N/A                  | ✅ Vulkan       |
| macOS        | ✅ Metal (dGPU)                | N/A            | ✅ Metal              | ✅ Metal (iGPU) |
| Web Browser  | ✅ WebGPU and WASM             |                |                      |                |
| iOS / iPadOS | ✅ Metal on Apple A-series GPU |                |                      |                |
| Android      | ✅ OpenCL on Adreno GPU        |                | ✅ OpenCL on Mali GPU |                |

然而我折腾半天，发现Windows下的支持不是很完善，Vulkan和CUDA都没法用，光用CPU这也太费电了。

发现有个好玩的[Ollama](https://github.com/ollama/ollama)可以比较方便的本地部署大预言模型，而且windows和linx下用CUDA加速也没问题。套一个[open-webui](https://github.com/open-webui/open-webui)的壳，就能像chatgpt那样，用浏览器和大模型聊天了。
